# -*- coding: utf-8 -*-
"""SentimentAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1faK_5kmExvI9laQvcLPsbXe73iW6kouM
"""

from google.colab import drive
drive.mount('/content/drive/')

import pandas as pd
dataset_path = '/content/drive/My Drive/DeepLearning/Dataset/dataset.csv'
DATASET_ENCODING = "ISO-8859-1"
DATASET_COLS = ["target", "id","date","flag","user","text"]
d = pd.read_csv(dataset_path, encoding=DATASET_ENCODING, names=DATASET_COLS)
d.head(5)

print("Dataset size:", len(d))

encode_target = {0:"NEGATIVE", 2:"NEUTRAL", 4:"POSITIVE"}
def encode_sentimentTarget(target):
  return encode_target[int(target)]

d.target = d.target.apply(lambda x: encode_sentimentTarget(x))

d.head(5)

import re
import nltk
from nltk.tokenize import word_tokenize
from string import punctuation 
nltk.download('stopwords')
from nltk.corpus import stopwords 
from nltk.stem import SnowballStemmer
import re
TextProcessingRE = "@\S+|https?:\S+|http?:\S|[^A-Za-z0-9]+"
_stopwords = stopwords.words("english")
_stemmer = SnowballStemmer("english")
def preprocess(text, stem=False):
  text = re.sub(TextProcessingRE,' ', str(text).lower()).strip()
  wordtokens = []
  for word in text.split():
    if word not in _stopwords:
      if stem:
        wordtokens.append(_stemmer.stem(word))
      else:
        wordtokens.append(word)
  return " ".join(wordtokens)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# d.text = d.text.apply(lambda x: preprocess(x))

d.head(5)

from sklearn.model_selection import train_test_split
train_size = 0.8
df_train, df_test = train_test_split(d, test_size=1-train_size, random_state=42)

print("Train Size:", len(df_train))
print("Test Size", len(df_test))

"""# Confirm TensorFlow has GPU"""

import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found!')
print('Found GPU at : {}'.format(device_name))

import gensim
#word2vec
w2v_size = 300
w2v_window = 7
w2v_epoch = 32
w2v_min_count = 10

# Commented out IPython magic to ensure Python compatibility.
# %%time
# documents = [_text.split() for _text in df_train.text]

!pip install gensim --upgrade

w2v_model = gensim.models.word2vec.Word2Vec(size=w2v_size, window=w2v_window, min_count=w2v_min_count, workers=8)

w2v_model.build_vocab(documents)

words = w2v_model.wv.vocab.keys()
vocab_size = len(words)
print("Vocab size:", vocab_size)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# w2v_model.train(documents, total_examples=len(documents), epochs=w2v_epoch)

w2v_model.most_similar("love")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from  tensorflow.keras.preprocessing.text import Tokenizer
# tokenizer = Tokenizer()
# tokenizer.fit_on_texts(df_train.text)
# vocab_size = len(tokenizer.word_index) + 1
# print("Total word", vocab_size)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from tensorflow.keras.preprocessing.sequence import pad_sequences
# max_len = 300
# x_train_sequence = tokenizer.texts_to_sequences(df_train.text)
# x_test_sequence = tokenizer.texts_to_sequences(df_test.text)
# x_train = pad_sequences(x_train_sequence, maxlen=max_len)
# x_test = pad_sequences(x_test_sequence, maxlen=max_len)
# print("Training X Shape:", x_train.shape)
# print("Testing X Shape:", x_test.shape)

labels = df_train.target.unique().tolist()
labels.append('NEUTRAL')

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
encoder.fit(df_train.target.tolist())
y_train = encoder.transform(df_train.target.tolist())
y_test = encoder.transform(df_test.target.tolist())
y_train = y_train.reshape(-1,1)
y_test = y_test.reshape(-1,1)
print("y_train", y_train.shape)
print("y_test", y_test.shape)

"""# Embedding Layer"""

import numpy as np
embedding_matrix = np.zeros((vocab_size, w2v_size))
for word, i in tokenizer.word_index.items():
  if word in w2v_model.wv:
    embedding_matrix[i] = w2v_model.wv[word]
print(embedding_matrix.shape)

from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding
embedding_layer = Embedding(vocab_size, w2v_size, weights=[embedding_matrix], input_length=max_len, trainable=False)

from tensorflow.keras.models import Sequential
model = Sequential()
model.add(embedding_layer)
model.add(Dropout(0.5))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.summary()

"""# Compile Model"""

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),
             EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# BATCH_SIZE = 1024
# EPOCHS = 5
# history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.1, verbose=1, callbacks=callbacks)

import pickle
TF_MODEL = 'model.h5'
WORD2VEC_MODEL = 'model.w2v'
TOKENIZER_MODEL = 'tokenizer.pkl'
ENCODER_MODEL = 'encoder.pkl'

model.save(TF_MODEL)
w2v_model.save(WORD2VEC_MODEL)
pickle.dump(tokenizer, open(TOKENIZER_MODEL, 'wb'), protocol=0)
pickle.dump(encoder, open(ENCODER_MODEL, 'wb'), protocol=0)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# score = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)
# print()
# print("")

print("Accuracy:", score[1])
print("Loss:", score[0])

#SENTIMENT
POSITIVE = "POSITIVE"
NEGATIVE = "NEGATIVE"
NEUTRAL = "NEUTRAL"
SENTIMENT_THRESHOLDS = (0.4, 0.7)
def decode_sentiment(score, include_neutral=True):
  if include_neutral:
    label = NEUTRAL
    if score <= SENTIMENT_THRESHOLDS[0]:
      label = NEGATIVE
    elif score >= SENTIMENT_THRESHOLDS[1]:
      label = POSITIVE
    return label
  else:
    return NEGATIVE if score < 0.5 else POSITIVE

import time
def predict(text, include_neutral=True):
  start_at = time.time()
  #tokenize text
  x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=max_len)
  score = new_model.predict([x_test])[0]
  label = decode_sentiment(score, include_neutral=include_neutral)
  return {"label": label, "score" : float(score), "elapsed_time": time.time()-start_at}

predict("I love the music")

predict("I hate rain")

predict("Every country has the right to prioritise the interest of its citizens.suspension might come as a jerk to India but when there isnt enough on the table to feed your own family, you dont invite guests. We just need to deal with that.")

import torch
path = F"/content/drive/My Drive/DeepLearning/{TF_MODEL}" 
model.save("/content/drive/My Drive/DeepLearning/model.h5")

# Recreate the exact same model, including its weights and the optimizer
new_model = tf.keras.models.load_model('/content/drive/My Drive/DeepLearning/model.h5')

# Show the model architecture
new_model.summary()

loss, acc = new_model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)
print('Restored model, accuracy: {:5.2f}%'.format(100*acc))